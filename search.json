[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Design of Experiments in R",
    "section": "",
    "text": "Welcome to the Design of Experiments in R with skpr DATAWorks 2023 workshop.\nSee the navigation bar on top to view the materials for the workshop."
  },
  {
    "objectID": "R_basics_tutorial.html",
    "href": "R_basics_tutorial.html",
    "title": "skpr_workshop_live_demo",
    "section": "",
    "text": "This document will help introduce"
  },
  {
    "objectID": "R_basics_tutorial.html#loading-libraries",
    "href": "R_basics_tutorial.html#loading-libraries",
    "title": "skpr_workshop_live_demo",
    "section": "Loading libraries",
    "text": "Loading libraries\n\nlibrary(skpr)\n\nLoading required package: shiny\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors"
  },
  {
    "objectID": "R_basics_tutorial.html#creating-numeric-vectors-with-c-and-seq",
    "href": "R_basics_tutorial.html#creating-numeric-vectors-with-c-and-seq",
    "title": "skpr_workshop_live_demo",
    "section": "Creating numeric vectors with c() and seq()",
    "text": "Creating numeric vectors with c() and seq()\n\naltitudes_manual = c(100, 200, 300)\naltitudes_manual\n\n[1] 100 200 300\n\nclass(altitudes_manual)\n\n[1] \"numeric\"\n\naltitudes_colon = 1:200\naltitudes_colon\n\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n[163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n[181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n[199] 199 200\n\naltitudes_seq_by = seq(from = 100, to = 1000, by = 5)\naltitudes_seq_by\n\n  [1]  100  105  110  115  120  125  130  135  140  145  150  155  160  165  170\n [16]  175  180  185  190  195  200  205  210  215  220  225  230  235  240  245\n [31]  250  255  260  265  270  275  280  285  290  295  300  305  310  315  320\n [46]  325  330  335  340  345  350  355  360  365  370  375  380  385  390  395\n [61]  400  405  410  415  420  425  430  435  440  445  450  455  460  465  470\n [76]  475  480  485  490  495  500  505  510  515  520  525  530  535  540  545\n [91]  550  555  560  565  570  575  580  585  590  595  600  605  610  615  620\n[106]  625  630  635  640  645  650  655  660  665  670  675  680  685  690  695\n[121]  700  705  710  715  720  725  730  735  740  745  750  755  760  765  770\n[136]  775  780  785  790  795  800  805  810  815  820  825  830  835  840  845\n[151]  850  855  860  865  870  875  880  885  890  895  900  905  910  915  920\n[166]  925  930  935  940  945  950  955  960  965  970  975  980  985  990  995\n[181] 1000\n\naltitudes_seq_length = seq(from = 50, to = 1000, length.out = 20)\naltitudes_seq_length\n\n [1]   50  100  150  200  250  300  350  400  450  500  550  600  650  700  750\n[16]  800  850  900  950 1000"
  },
  {
    "objectID": "R_basics_tutorial.html#creating-character-vectors-with-c-and-paste",
    "href": "R_basics_tutorial.html#creating-character-vectors-with-c-and-paste",
    "title": "skpr_workshop_live_demo",
    "section": "Creating character vectors with c() and paste()",
    "text": "Creating character vectors with c() and paste()\n\nmodes = c(\"low\",\"medium\",\"high\")\nmodes\n\n[1] \"low\"    \"medium\" \"high\"  \n\nclass(modes)\n\n[1] \"character\"\n\nmodes_paste = paste(\"mode\", c(\"A\",\"B\",\"C\",\"D\",\"E\"))\nmodes_paste\n\n[1] \"mode A\" \"mode B\" \"mode C\" \"mode D\" \"mode E\"\n\nmodes_paste = paste(\"mode\", c(\"A\",\"B\",\"C\",\"D\",\"E\"), sep=\"_\")\nmodes_paste\n\n[1] \"mode_A\" \"mode_B\" \"mode_C\" \"mode_D\" \"mode_E\"\n\nmodes_paste0 = paste0(\"mode\", 1:5)\nmodes_paste0\n\n[1] \"mode1\" \"mode2\" \"mode3\" \"mode4\" \"mode5\""
  },
  {
    "objectID": "R_basics_tutorial.html#creating-factors-from-character-vectors-matters-for-dummy-encoding",
    "href": "R_basics_tutorial.html#creating-factors-from-character-vectors-matters-for-dummy-encoding",
    "title": "skpr_workshop_live_demo",
    "section": "Creating factors from character vectors (Matters for dummy encoding)",
    "text": "Creating factors from character vectors (Matters for dummy encoding)\n\nmodes_factor = factor(c(\"low\",\"medium\",\"high\"))\nmodes_factor\n\n[1] low    medium high  \nLevels: high low medium\n\ncontr.treatment(modes_factor)\n\n       medium high\nlow         0    0\nmedium      1    0\nhigh        0    1\n\nmodes_factor_levels = factor(c(\"low\",\"medium\", \"off\", \"high\"),\n                             levels = c(\"off\", \"low\", \"medium\", \"high\"))\ncontr.treatment(modes_factor_levels)\n\n       medium off high\nlow         0   0    0\nmedium      1   0    0\noff         0   1    0\nhigh        0   0    1"
  },
  {
    "objectID": "R_basics_tutorial.html#creating-candidate-sets-with-expand.grid",
    "href": "R_basics_tutorial.html#creating-candidate-sets-with-expand.grid",
    "title": "skpr_workshop_live_demo",
    "section": "Creating candidate sets with expand.grid()",
    "text": "Creating candidate sets with expand.grid()\n\ncandidate_set = expand.grid(altitudes = c(100, 200, 300),\n                            mode = c(\"low\", \"high\"),\n                            operator = c(\"blue\", \"gold\"))\ncandidate_set\n\n   altitudes mode operator\n1        100  low     blue\n2        200  low     blue\n3        300  low     blue\n4        100 high     blue\n5        200 high     blue\n6        300 high     blue\n7        100  low     gold\n8        200  low     gold\n9        300  low     gold\n10       100 high     gold\n11       200 high     gold\n12       300 high     gold"
  },
  {
    "objectID": "skpr_eval_design_live_demo.html",
    "href": "skpr_eval_design_live_demo.html",
    "title": "Design Evaluation",
    "section": "",
    "text": "Load our libraries\n\nlibrary(skpr)\nlibrary(tidyverse)\n#Set default contrasts to sum contrasts\noptions(contrasts = rep(\"contr.sum\", 2))     \n\n\n\nLoading our design\nskpr designs are R data.frames, so we can load in an external CSV file and skpr can use that as a design. We’ll also make sure our candidate set is available to generate new designs as well.\n\ncandidate_set = expand.grid(altitude    = seq(10000,30000,by=1000),\n                            speed       = seq(450,550,by=5),\n                            mode        = c(\"scan\", \"strip\", \"spotlight\"),\n                            target_env = c(\"urban\", \"desert\"))\n\nconstrained_candidate_set = candidate_set |> \n  filter(70000 - speed * 90 > altitude) |> \n  filter(-430000 + speed * 1000 > altitude) |> \n  filter(150000 - speed * 300 < altitude)  |> \n  filter(-14000 + speed * 50 < altitude)\n\ndesign_final = read.csv(\"design_final.csv\")\ndesign_final\n\n   altitude speed      mode\n1     20000   550      scan\n2     28000   460     strip\n3     14000   550      scan\n4     10000   470 spotlight\n5     28000   460     strip\n6     28000   460 spotlight\n7     10000   470      scan\n8     20000   550     strip\n9     14000   550      scan\n10    28000   460      scan\n11    28000   460     strip\n12    10000   470 spotlight\n13    20000   550      scan\n14    14000   550     strip\n15    20000   550 spotlight\n16    10000   470      scan\n17    28000   460 spotlight\n18    10000   470     strip\n19    18000   450 spotlight\n20    20000   550 spotlight\n21    28000   460      scan\n22    10000   470 spotlight\n23    10000   470     strip\n24    28000   460     strip\n25    20000   550      scan\n26    28000   460 spotlight\n27    14000   550 spotlight\n28    20000   550     strip\n29    14000   550      scan\n30    24000   510      scan\n31    19000   450     strip\n32    10000   470 spotlight\n33    14000   550     strip\n34    28000   460      scan\n35    19000   450      scan\n36    10000   470      scan\n37    20000   550 spotlight\n38    28000   460 spotlight\n39    19000   450      scan\n40    14000   550 spotlight\n41    10000   470     strip\n42    24000   510     strip\n43    14000   550     strip\n44    10000   470 spotlight\n45    19000   450 spotlight\n46    14000   550      scan\n47    28000   460     strip\n48    24000   510 spotlight\n49    20000   550     strip\n50    28000   460      scan\n51    28000   460      scan\n52    10000   470      scan\n53    16000   450      scan\n54    14000   550 spotlight\n55    16000   450 spotlight\n56    18000   500 spotlight\n57    24000   510      scan\n58    16000   450     strip\n59    12000   515      scan\n60    28000   460 spotlight\n61    10000   470     strip\n62    18000   495      scan\n63    20000   550 spotlight\n64    18000   500 spotlight\n65    19000   450     strip\n66    14000   550     strip\n67    18000   495     strip\n68    24000   510     strip\n69    10000   470     strip\n70    10000   470      scan\n71    12000   515     strip\n72    12000   515 spotlight\n\n\nLet’s first start by talking about our response variable. We are going to treat our NIIRS rating as a continuous response, and we want to characterize how that rating changes as a function of altitude, mode, and speed. Our model includes a quadratic term to see if there is any curvature present. Let’s run this power analysis with eval_design().\nOur goal is 80% power in all model terms. Let’s see how we do, with the assumption that we are going to fit a linear model to the data and our design is fairly well balanced.\n\neval_design(design = design_final,\n            model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n            alpha = 0.05,\n            effectsize = 2)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.7672975\n2        altitude    effect.power 0.9713374\n3           speed    effect.power 0.9983038\n4            mode    effect.power 0.9999913\n5      I(speed^2)    effect.power 0.7122924\n6   I(altitude^2)    effect.power 0.7392905\n7  altitude:speed    effect.power 0.7345717\n8   altitude:mode    effect.power 0.9922116\n9      speed:mode    effect.power 0.9982285\n10    (Intercept) parameter.power 0.7672975\n11       altitude parameter.power 0.9713374\n12          speed parameter.power 0.9983038\n13          mode1 parameter.power 0.9999368\n14          mode2 parameter.power 0.9999348\n15     I(speed^2) parameter.power 0.7122924\n16  I(altitude^2) parameter.power 0.7392905\n17 altitude:speed parameter.power 0.7345717\n18 altitude:mode1 parameter.power 0.9857589\n19 altitude:mode2 parameter.power 0.9842852\n20    speed:mode1 parameter.power 0.9953791\n21    speed:mode2 parameter.power 0.9954601\n==============Evaluation Info==============\n* Alpha = 0.05 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode + I(speed^2) + I(altitude^2) \n* Anticipated Coefficients = c(1, 1, 1, 1, -1, 1, 1, 1, 1, -1, 1, -1) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\nHere we see our design and model, but we also have two new arguments: alpha and effectsize. alpha is the Type-I error, and effectsize is here stated as an signal to noise ratio (SNR) of 2. Let’s look at the slides for some more information on effect size.\nLet’s say we know (based on previous tests) to expect an intrinsic variation of about 1 NIIRS point from run to run, independent of the experimental factors. An effect size of 2 then corresponds to being able to detect a difference of 2 NIIRS points. Let’s also say that we must confirm that our system produces images within 1 point on the NIIRS scale–our effect size should then be halved.\n\neval_design(design = design_final,\n            model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n            alpha = 0.05,\n            effectsize = 1)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.2698055\n2        altitude    effect.power 0.4883863\n3           speed    effect.power 0.6863168\n4            mode    effect.power 0.8517509\n5      I(speed^2)    effect.power 0.2426662\n6   I(altitude^2)    effect.power 0.2554208\n7  altitude:speed    effect.power 0.2531190\n8   altitude:mode    effect.power 0.5507805\n9      speed:mode    effect.power 0.6441155\n10    (Intercept) parameter.power 0.2698055\n11       altitude parameter.power 0.4883863\n12          speed parameter.power 0.6863168\n13          mode1 parameter.power 0.8257357\n14          mode2 parameter.power 0.8247416\n15     I(speed^2) parameter.power 0.2426662\n16  I(altitude^2) parameter.power 0.2554208\n17 altitude:speed parameter.power 0.2531190\n18 altitude:mode1 parameter.power 0.5460779\n19 altitude:mode2 parameter.power 0.5383437\n20    speed:mode1 parameter.power 0.6262726\n21    speed:mode2 parameter.power 0.6274187\n==============Evaluation Info==============\n* Alpha = 0.05 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode + I(speed^2) + I(altitude^2) \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\nHere, let’s say our organization is willing to accept the risk that we call the system effective at 80% confidence instead of 95%: This means we will set our alpha to 0.2. This change (along with requiring 80% power) means we are saying we will accept either a false positive or false negative with equal probability.\n\neval_design(design = design_final,\n            model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n            alpha = 0.2,\n            effectsize = 1)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.5344564\n2        altitude    effect.power 0.7482697\n3           speed    effect.power 0.8824046\n4            mode    effect.power 0.9612043\n5      I(speed^2)    effect.power 0.5014520\n6   I(altitude^2)    effect.power 0.5171805\n7  altitude:speed    effect.power 0.5143710\n8   altitude:mode    effect.power 0.8012921\n9      speed:mode    effect.power 0.8617164\n10    (Intercept) parameter.power 0.5344564\n11       altitude parameter.power 0.7482697\n12          speed parameter.power 0.8824046\n13          mode1 parameter.power 0.9498182\n14          mode2 parameter.power 0.9494140\n15     I(speed^2) parameter.power 0.5014520\n16  I(altitude^2) parameter.power 0.5171805\n17 altitude:speed parameter.power 0.5143710\n18 altitude:mode1 parameter.power 0.7923765\n19 altitude:mode2 parameter.power 0.7867171\n20    speed:mode1 parameter.power 0.8466934\n21    speed:mode2 parameter.power 0.8474138\n==============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode + I(speed^2) + I(altitude^2) \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\nWe see here we still don’t have enough power in some of the terms–let’s generate some new designs and see how many runs we need in order to reach 80% power everywhere.\n\nset.seed(2023)\ngen_design(candidateset = constrained_candidate_set,\n           model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n           trials = 100) |> \n  eval_design(alpha=0.2, effectsize = 1)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.6282153\n2        altitude    effect.power 0.8433387\n3           speed    effect.power 0.9559134\n4            mode    effect.power 0.9907754\n5      I(speed^2)    effect.power 0.5978099\n6   I(altitude^2)    effect.power 0.6084643\n7  altitude:speed    effect.power 0.6047212\n8   altitude:mode    effect.power 0.8891265\n9      speed:mode    effect.power 0.9433138\n10    (Intercept) parameter.power 0.6282153\n11       altitude parameter.power 0.8433387\n12          speed parameter.power 0.9559134\n13          mode1 parameter.power 0.9851315\n14          mode2 parameter.power 0.9851012\n15     I(speed^2) parameter.power 0.5978099\n16  I(altitude^2) parameter.power 0.6084643\n17 altitude:speed parameter.power 0.6047212\n18 altitude:mode1 parameter.power 0.8761100\n19 altitude:mode2 parameter.power 0.8708975\n20    speed:mode1 parameter.power 0.9283013\n21    speed:mode2 parameter.power 0.9312056\n==============Evaluation Info==============\n* Alpha = 0.2 * Trials = 100 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode + I(speed^2) + I(altitude^2) \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\nset.seed(2023)\ngen_design(candidateset = constrained_candidate_set,\n           model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n           trials = 150) |> \n  eval_design(alpha=0.2, effectsize = 1)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.7501838\n2        altitude    effect.power 0.9403180\n3           speed    effect.power 0.9922560\n4            mode    effect.power 0.9993906\n5      I(speed^2)    effect.power 0.7211095\n6   I(altitude^2)    effect.power 0.7310307\n7  altitude:speed    effect.power 0.7330595\n8   altitude:mode    effect.power 0.9669516\n9      speed:mode    effect.power 0.9882621\n10    (Intercept) parameter.power 0.7501838\n11       altitude parameter.power 0.9403180\n12          speed parameter.power 0.9922560\n13          mode1 parameter.power 0.9983660\n14          mode2 parameter.power 0.9984922\n15     I(speed^2) parameter.power 0.7211095\n16  I(altitude^2) parameter.power 0.7310307\n17 altitude:speed parameter.power 0.7330595\n18 altitude:mode1 parameter.power 0.9560368\n19 altitude:mode2 parameter.power 0.9550857\n20    speed:mode1 parameter.power 0.9807848\n21    speed:mode2 parameter.power 0.9821939\n==============Evaluation Info==============\n* Alpha = 0.2 * Trials = 150 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode + I(speed^2) + I(altitude^2) \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\nset.seed(2023)\ngen_design(candidateset = constrained_candidate_set,\n           model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n           trials = 200) |> \n  eval_design(alpha=0.2, effectsize = 1)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.8279661\n2        altitude    effect.power 0.9780989\n3           speed    effect.power 0.9984155\n4            mode    effect.power 0.9999690\n5      I(speed^2)    effect.power 0.8017438\n6   I(altitude^2)    effect.power 0.8120091\n7  altitude:speed    effect.power 0.8209540\n8   altitude:mode    effect.power 0.9916157\n9      speed:mode    effect.power 0.9978725\n10    (Intercept) parameter.power 0.8279661\n11       altitude parameter.power 0.9780989\n12          speed parameter.power 0.9984155\n13          mode1 parameter.power 0.9998473\n14          mode2 parameter.power 0.9998629\n15     I(speed^2) parameter.power 0.8017438\n16  I(altitude^2) parameter.power 0.8120091\n17 altitude:speed parameter.power 0.8209540\n18 altitude:mode1 parameter.power 0.9855181\n19 altitude:mode2 parameter.power 0.9860742\n20    speed:mode1 parameter.power 0.9952756\n21    speed:mode2 parameter.power 0.9956503\n==============Evaluation Info==============\n* Alpha = 0.2 * Trials = 200 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode + I(speed^2) + I(altitude^2) \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\nWe can also include the effect of blocking terms by setting blocking = TRUE. This accounts for the split-plot structure and allows more accurate estimates of power for the hard-to-change terms, which will usually have lower power than the easy-to-change terms in the model.\n\nhtc_design = gen_design(candidateset = constrained_candidate_set,\n                        model = ~target_env,\n                        trials=10) \n\ngen_design(candidateset = constrained_candidate_set,\n             model = ~ (altitude + speed + mode + target_env)^2 + I(speed^2) + I(altitude^2),\n             trials = 200, parallel = FALSE, splitplotdesign = htc_design, repeats = 1) ->\nsplit_plot_design \n\n\n#Compare estimates \neval_design(split_plot_design, alpha = 0.2, effectsize = 1)\n\n              parameter            type     power\n1           (Intercept)    effect.power 0.4699707\n2              altitude    effect.power 0.9755689\n3                 speed    effect.power 0.9977338\n4                  mode    effect.power 0.9999651\n5            target_env    effect.power 0.5740785\n6            I(speed^2)    effect.power 0.7680466\n7         I(altitude^2)    effect.power 0.7856904\n8        altitude:speed    effect.power 0.8159483\n9         altitude:mode    effect.power 0.9930435\n10  altitude:target_env    effect.power 0.9999054\n11           speed:mode    effect.power 0.9979582\n12     speed:target_env    effect.power 0.9999898\n13      mode:target_env    effect.power 0.9999756\n14          (Intercept) parameter.power 0.4699707\n15             altitude parameter.power 0.9755689\n16                speed parameter.power 0.9977338\n17                mode1 parameter.power 0.9998445\n18                mode2 parameter.power 0.9998459\n19          target_env1 parameter.power 0.5740785\n20           I(speed^2) parameter.power 0.7680466\n21        I(altitude^2) parameter.power 0.7856904\n22       altitude:speed parameter.power 0.8159483\n23       altitude:mode1 parameter.power 0.9880052\n24       altitude:mode2 parameter.power 0.9879377\n25 altitude:target_env1 parameter.power 0.9999054\n26          speed:mode1 parameter.power 0.9956841\n27          speed:mode2 parameter.power 0.9956403\n28    speed:target_env1 parameter.power 0.9999898\n29    mode1:target_env1 parameter.power 0.9998848\n30    mode2:target_env1 parameter.power 0.9998852\n=================Evaluation Info=================\n* Alpha = 0.2 * Trials = 200 * Blocked = TRUE \n* Evaluating Model = ~altitude + speed + mode + target_env + altitude:speed + altitude:mode + altitude:target_env + speed:mode + speed:target_env + mode:target_env + I(speed^2) + I(altitude^2) \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, -0.500, 0.500, 0.500, -0.500) \n* Number of Blocks = Level 1: 10 \n* Variance Ratios  = Level 1: 1, Level 2: 1 \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lme4::lmer(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\n\n\nCalculate Power Curves\nskpr provides an easier and more informative method of exploring power than iteratively walking though different design sizes manually: the calculate_power_curves() function. This function automatically generates an optimal design for a given candidate set and number of runs, and plots the resulting power curves across all powers and effect sizes. Let’s see it in action:\n\npower_values = calculate_power_curves(trials = seq(10,200,by=10),\n                       candidateset = constrained_candidate_set,\n                       model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n                       effectsize = 1,\n                       x_breaks =  seq(10,200,by=10),\n                       alpha = 0.2,\n                       ggplot_elements = list(geom_hline(yintercept = 0.8, \n                                                         alpha = 0.5, \n                                                         linetype = \"dashed\", \n                                                         color = \"red\", \n                                                         linewidth = 1)))\n\n\n\n\nThis graph tells us a great deal about our test resource/model-complexity trade space: the two clustered groups of power curves delineates a clear decision we can make regarding resource allocation. If we aren’t actually interested in modelling curvature, we will have 80% power in all main effect terms and all interactions except altitude:speed at about 90 runs. Otherwise, we need to more than double our test resources to 200 runs to have an adequate test. Is 2x the number of experimental runs worth modeling curvature? Let’s say we have theoretical or historical justification that says it isn’t, and re-run our analysis with a smaller model.\n\npower_values = calculate_power_curves(trials = seq(10,200,by=10),\n                       candidateset = constrained_candidate_set,\n                       model = ~ (altitude + speed + mode)^2,\n                       effectsize = 1,\n                       x_breaks =  seq(10,200,by=10),\n                       alpha = 0.2,\n                       ggplot_elements = list(geom_hline(yintercept = 0.8, \n                                                         alpha = 0.5, \n                                                         linetype = \"dashed\", \n                                                         color = \"red\", \n                                                         linewidth = 1)))\n\n\n\n\nWe now see we need about 120 runs to have a well-powered test for all main effects and interactions. Note that by removing the quadratic terms, the number of runs to estimate the altitude:speed interaction decreased greatly, from 200 to 120 runs. This shows that small changes to the model can have dramatic effects on power, even when you’re dealing with a relatively large test.\nYou might have also noticed we have two different types of powers reported: “effect.power” and “parameter.power”. What’s the difference? Let’s see the slides.\nTo better explain this, let’s make up some fake results for our test and simulate conducting an analysis to demonstrate in real-world terms what these types of power are actually referring to. We’ll generate some fake data and fit a linear model (using lm()) and . For both parameter and effect power, we need to specify a model: here, we’ll include all main effects and 2nd order interactions. Since we’re including interactions in our model, we should normalize the numeric terms in our model to -1 to 1, which helps prevent correlation between terms. We can do this with the skpr helper function normalize_design().\n\nset.seed(123)\ndesign_90runs = gen_design(candidateset = constrained_candidate_set,\n                            model = ~ (altitude + speed + mode)^2,\n                            trials = 72)\n\neval_design(design_90runs, effectsize = 1, alpha=0.2)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.9883613\n2        altitude    effect.power 0.7050435\n3           speed    effect.power 0.9913139\n4            mode    effect.power 0.9622582\n5  altitude:speed    effect.power 0.6500264\n6   altitude:mode    effect.power 0.8670089\n7      speed:mode    effect.power 0.9380883\n8     (Intercept) parameter.power 0.9883613\n9        altitude parameter.power 0.7050435\n10          speed parameter.power 0.9913139\n11          mode1 parameter.power 0.9506650\n12          mode2 parameter.power 0.9506650\n13 altitude:speed parameter.power 0.6500264\n14 altitude:mode1 parameter.power 0.8522629\n15 altitude:mode2 parameter.power 0.8522629\n16    speed:mode1 parameter.power 0.9243885\n17    speed:mode2 parameter.power 0.9243885\n==============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n#Note the dramatic improvement in correlation when you standardize the numeric columns\nplot_correlations(design_90runs, standardize = FALSE)\n\n\n\nplot_correlations(design_90runs)\n\n\n\n#So we will always normalize\nsimulated_results_design = normalize_design(design_90runs)\nset.seed(123)\n#Create fake NIIRS results (where null hypothesis is true--no factor influences the response)\nsimulated_results_design$NIIRS = rnorm(n = nrow(design_90runs), sd = 1, mean = 4)\nsimulated_results_design\n\n     altitude      speed      mode    NIIRS\n1  -0.5555556  1.0000000      scan 3.439524\n2   1.0000000 -1.0000000      scan 3.769823\n3  -0.5555556  1.0000000 spotlight 5.558708\n4   1.0000000 -1.0000000 spotlight 4.070508\n5  -0.5555556  1.0000000     strip 4.129288\n6   0.1111111  1.0000000     strip 5.715065\n7   0.1111111  1.0000000 spotlight 4.460916\n8  -0.5555556  1.0000000      scan 2.734939\n9   0.1111111  1.0000000      scan 3.313147\n10  1.0000000 -1.0000000      scan 3.554338\n11  1.0000000 -1.0000000     strip 5.224082\n12 -1.0000000 -0.7777778     strip 4.359814\n13  1.0000000 -1.0000000 spotlight 4.400771\n14  1.0000000 -1.0000000 spotlight 4.110683\n15 -0.5555556  1.0000000 spotlight 3.444159\n16 -1.0000000 -0.7777778      scan 5.786913\n17 -1.0000000 -0.7777778     strip 4.497850\n18 -0.5555556  1.0000000     strip 2.033383\n19  0.1111111  1.0000000 spotlight 4.701356\n20  1.0000000 -1.0000000      scan 3.527209\n21  0.1111111  1.0000000     strip 2.932176\n22  1.0000000 -1.0000000     strip 3.782025\n23  0.1111111  1.0000000     strip 2.973996\n24 -0.5555556  1.0000000      scan 3.271109\n25 -0.5555556  1.0000000 spotlight 3.374961\n26 -1.0000000 -0.7777778 spotlight 2.313307\n27  1.0000000 -1.0000000 spotlight 4.837787\n28 -1.0000000 -0.7777778 spotlight 4.153373\n29  0.1111111  1.0000000      scan 2.861863\n30  1.0000000 -1.0000000      scan 5.253815\n31 -0.5555556  1.0000000     strip 4.426464\n32  1.0000000 -1.0000000 spotlight 3.704929\n33 -1.0000000 -0.7777778     strip 4.895126\n34  1.0000000 -1.0000000     strip 4.878133\n35 -1.0000000 -0.7777778      scan 4.821581\n36  0.1111111  1.0000000 spotlight 4.688640\n37 -0.5555556  1.0000000      scan 4.553918\n38  0.1111111  1.0000000      scan 3.938088\n39  1.0000000 -1.0000000     strip 3.694037\n40  1.0000000 -1.0000000     strip 3.619529\n41 -1.0000000 -0.7777778      scan 3.305293\n42 -1.0000000 -0.7777778 spotlight 3.792083\n43 -1.0000000 -0.7777778 spotlight 2.734604\n44  1.0000000 -1.0000000 spotlight 6.168956\n45 -0.5555556  1.0000000 spotlight 5.207962\n46 -0.5555556  1.0000000     strip 2.876891\n47 -1.0000000 -0.7777778     strip 3.597115\n48 -1.0000000 -0.7777778     strip 3.533345\n49  0.1111111  1.0000000 spotlight 4.779965\n50  1.0000000 -1.0000000      scan 3.916631\n51  0.1111111  1.0000000     strip 4.253319\n52 -1.0000000 -0.7777778      scan 3.971453\n53 -1.0000000 -0.7777778     strip 3.957130\n54 -0.5555556  1.0000000      scan 5.368602\n55 -1.0000000 -0.7777778 spotlight 3.774229\n56  0.1111111  1.0000000      scan 5.516471\n57 -1.0000000 -0.7777778 spotlight 2.451247\n58  1.0000000 -1.0000000     strip 4.584614\n59  1.0000000 -1.0000000      scan 4.123854\n60  1.0000000 -1.0000000 spotlight 4.215942\n61  0.1111111  1.0000000     strip 4.379639\n62  0.1111111  1.0000000      scan 3.497677\n63  0.1111111  1.0000000 spotlight 3.666793\n64 -1.0000000 -0.7777778      scan 2.981425\n65 -0.5555556  1.0000000 spotlight 2.928209\n66  1.0000000 -1.0000000      scan 4.303529\n67 -1.0000000 -0.7777778      scan 4.448210\n68 -0.5555556  1.0000000     strip 4.053004\n69  1.0000000 -1.0000000     strip 4.922267\n70 -1.0000000 -0.7777778     strip 6.050085\n71 -1.0000000 -0.7777778      scan 3.508969\n72 -1.0000000 -0.7777778 spotlight 1.690831\n\nlm(formula =  NIIRS ~ (altitude + speed + mode)^2,\n           data = simulated_results_design) |> \n  summary()\n\n\nCall:\nlm(formula = NIIRS ~ (altitude + speed + mode)^2, data = simulated_results_design)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6837 -0.6327 -0.1341  0.4997  1.8692 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.07767    0.12514  32.584   <2e-16 ***\naltitude        0.32708    0.24444   1.338   0.1858    \nspeed          -0.01326    0.12139  -0.109   0.9133    \nmode1          -0.08056    0.15150  -0.532   0.5968    \nmode2           0.05562    0.15150   0.367   0.7148    \naltitude:speed  0.10071    0.26725   0.377   0.7076    \naltitude:mode1 -0.29413    0.19094  -1.540   0.1285    \naltitude:mode2 -0.23470    0.19094  -1.229   0.2236    \nspeed:mode1    -0.10597    0.16356  -0.648   0.5194    \nspeed:mode2    -0.29564    0.16356  -1.808   0.0755 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8946 on 62 degrees of freedom\nMultiple R-squared:  0.2085,    Adjusted R-squared:  0.09366 \nF-statistic: 1.815 on 9 and 62 DF,  p-value: 0.08322\n\n\nFor parameter power, we see we get p-values telling us the significance of each factor. Importantly, regression also gives us coefficients for each model term, which allows us to characterize performance across the test space and make statements like “Traveling at speed X and altitude Y decreases or increases performance by N NIIRS points”.\nEffect power looks at the significance of the model terms themselves. Running an ANOVA is a subset of regression–we still fit a linear model–but ANOVA looks to see how much of the total variance is explained by each of the predictors.\n\n#Now let's look at effect power--first, Type-III ANOVA\nlm(formula =  NIIRS ~ (altitude + speed + mode)^2,\n   data = simulated_results_design) |> \n  car::Anova(type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: NIIRS\n               Sum Sq Df   F value  Pr(>F)    \n(Intercept)    849.70  1 1061.6958 < 2e-16 ***\naltitude         1.43  1    1.7904 0.18576    \nspeed            0.01  1    0.0119 0.91334    \nmode             0.24  2    0.1482 0.86254    \naltitude:speed   0.11  1    0.1420 0.70757    \naltitude:mode    6.17  2    3.8516 0.02650 *  \nspeed:mode       5.18  2    3.2387 0.04594 *  \nResiduals       49.62 62                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSimilarly, you can do a likelihood ratio test between two models (one with and one without the term in question) and see if the difference is significant.\n\n#Example: p-value for altitude:mode is approximately 0.76--let's see what it is under a likelihood ratio test\n\nformula_full =  NIIRS ~ (altitude + speed + mode)^2 \nformula_reduced =  NIIRS ~ (altitude + speed + mode)^2 - altitude:mode\n\nfit_full = lm(formula =  formula_full, data = simulated_results_design)\nfit_reduced = lm(formula =  formula_reduced, data = simulated_results_design)\n\n#Compare the outputs of the two models\ndiffobj::diffPrint(summary(fit_full),summary(fit_reduced), interactive = FALSE)\n\n< summary(fit_full)                      > summary(fit_reduced)                 \n@@ 1,35 @@                               @@ 1,31 @@                             \n                                                                                \n  Call:                                    Call:                                \n< lm(formula = formula_full, data = sim  > lm(formula = formula_reduced, data = \n: ulated_results_design)                 : simulated_results_design)            \n                                                                                \n  Residuals:                               Residuals:                           \n      Min      1Q  Median      3Q               Min       1Q   Median       3Q  \n< -1.6837 -0.6327 -0.1341  0.4997        > -1.88022 -0.71321  0.03328  0.44536  \n      Max                                       Max                             \n<  1.8692                                >  2.18666                             \n                                                                                \n  Coefficients:                            Coefficients:                        \n                 Estimate Std. Error                      Estimate Std. Error   \n< (Intercept)     4.07767    0.12514     > (Intercept)     4.07767    0.13060   \n< altitude        0.32708    0.24444     > altitude        0.32708    0.25510   \n< speed          -0.01326    0.12139     > speed          -0.01326    0.12668   \n< mode1          -0.08056    0.15150     > mode1          -0.04761    0.15652   \n< mode2           0.05562    0.15150     > mode2           0.08191    0.15652   \n< altitude:speed  0.10071    0.26725     > altitude:speed  0.10071    0.27891   \n< altitude:mode1 -0.29413    0.19094     ~                                      \n< altitude:mode2 -0.23470    0.19094     ~                                      \n< speed:mode1    -0.10597    0.16356     > speed:mode1    -0.04985    0.16640   \n< speed:mode2    -0.29564    0.16356     > speed:mode2    -0.25086    0.16640   \n                 t value Pr(>|t|)                         t value Pr(>|t|)      \n< (Intercept)     32.584   <2e-16 ***    > (Intercept)     31.222   <2e-16 ***  \n< altitude         1.338   0.1858        > altitude         1.282    0.204      \n< speed           -0.109   0.9133        > speed           -0.105    0.917      \n< mode1           -0.532   0.5968        > mode1           -0.304    0.762      \n< mode2            0.367   0.7148        > mode2            0.523    0.603      \n< altitude:speed   0.377   0.7076        > altitude:speed   0.361    0.719      \n< altitude:mode1  -1.540   0.1285        ~                                      \n< altitude:mode2  -1.229   0.2236        ~                                      \n< speed:mode1     -0.648   0.5194        > speed:mode1     -0.300    0.765      \n< speed:mode2     -1.808   0.0755 .      > speed:mode2     -1.508    0.137      \n  ---                                      ---                                  \n  Signif. codes:                           Signif. codes:                       \n@@ 37,6 @@                               @@ 33,6 @@                             \n    '.' 0.1 ' ' 1                            '.' 0.1 ' ' 1                      \n                                                                                \n< Residual standard error: 0.8946 on 62  > Residual standard error: 0.9336 on 64\n:  degrees of freedom                    :  degrees of freedom                  \n< Multiple R-squared:  0.2085,    Adjus  > Multiple R-squared:  0.1102,    Adjus\n: ted R-squared:  0.09366                : ted R-squared:  0.01289              \n< F-statistic: 1.815 on 9 and 62 DF,  p  > F-statistic: 1.132 on 7 and 64 DF,  p\n: -value: 0.08322                        : -value: 0.3543                       \n                                                                                \n\nlmtest::lrtest(fit_full, fit_reduced)\n\nLikelihood ratio test\n\nModel 1: NIIRS ~ (altitude + speed + mode)^2\nModel 2: NIIRS ~ (altitude + speed + mode)^2 - altitude:mode\n  #Df  LogLik Df  Chisq Pr(>Chisq)  \n1  11 -88.762                       \n2   9 -92.978 -2 8.4321    0.01476 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice we had some significant effects in both effect and parameter terms in our simulated model fit, even though we were fitting nothing but noise. Were these real, or just a result of our acceptable type-I error rate of 20%? To figure this out, we can repeat the process of fitting our design with noise and see if we end up with 20% of the runs being marked as significant by chance. We’ll do this with a simple for loop that repeatedly generates noise and fits the model and counts the number of times each term is marked as significant, and then divide this number by the total number of simulations.\n\nlm(formula =  NIIRS ~ (altitude + speed + mode)^2,\n             data = simulated_results_design) |> \n    summary() |> \n  coef() |> \n  rownames() ->\nmodel_terms\n#See model terms\nmodel_terms\n\n [1] \"(Intercept)\"    \"altitude\"       \"speed\"          \"mode1\"         \n [5] \"mode2\"          \"altitude:speed\" \"altitude:mode1\" \"altitude:mode2\"\n [9] \"speed:mode1\"    \"speed:mode2\"   \n\nis_significant = rep(0,length(model_terms))\nnames(is_significant) = model_terms\nis_significant\n\n   (Intercept)       altitude          speed          mode1          mode2 \n             0              0              0              0              0 \naltitude:speed altitude:mode1 altitude:mode2    speed:mode1    speed:mode2 \n             0              0              0              0              0 \n\nfor(i in 1:1000) {\n  set.seed(i)\n  #Create fake NIIRS results (where null hypothesis is true--no factor influences the response)\n  simulated_results_design$NIIRS = rnorm(n = nrow(design_90runs), sd = 1, mean = 4)\n  \n  lm(formula =  NIIRS ~ (altitude + speed + mode)^2,\n             data = simulated_results_design) |> \n    summary() |> \n    coef() ->\n  fit_coefficients \n  pvals = fit_coefficients[,4] #p-values\n  \n  #Add 1 to the term if significant\n  is_significant = is_significant + ifelse(pvals < 0.2, 1, 0) \n}\n#Without normalizing to the number of runs\nis_significant\n\n   (Intercept)       altitude          speed          mode1          mode2 \n          1000            176            212            207            189 \naltitude:speed altitude:mode1 altitude:mode2    speed:mode1    speed:mode2 \n           201            204            187            196            210 \n\n#Normalized\nis_significant/1000\n\n   (Intercept)       altitude          speed          mode1          mode2 \n         1.000          0.176          0.212          0.207          0.189 \naltitude:speed altitude:mode1 altitude:mode2    speed:mode1    speed:mode2 \n         0.201          0.204          0.187          0.196          0.210 \n\n\nWe see that our model fitting process indeed marks all terms as significant about 20% of the time, as expected given our Type-I error value.\nWhat if we actually “activated” some of these effects? We can do this by shifting the simulated resulting NIIRS score proportional to the factors in the design matrix. Since the design is coded from -1 to 1 (a width of 2), we’ll set the coefficients for speed, altitude, and the interaction between speed and altitude to 0.5 to get an effect size of 1. This means that when the altitude, speed, and altitude:speed is at their highest values, we will see a shift of 1 NIIRS point compared to when they are at their lowest.\n\nset.seed(143)\n#Create fake NIIRS results (where altitude and speed and altitude:speed are active effects)\nsimulated_results_design$NIIRS = rnorm(n = nrow(design_90runs), sd = 1, mean = 4) + \n  0.5 * simulated_results_design$altitude + \n  0.5 * simulated_results_design$speed +\n  0.5 * (simulated_results_design$speed * simulated_results_design$altitude)\n\nlm(formula =  NIIRS ~ (altitude + speed + mode)^2,\n           data = simulated_results_design) |> \n  summary()\n\n\nCall:\nlm(formula = NIIRS ~ (altitude + speed + mode)^2, data = simulated_results_design)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.84344 -0.83847  0.05234  0.79814  2.19733 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.27792    0.14735  29.032  < 2e-16 ***\naltitude        0.89446    0.28782   3.108  0.00284 ** \nspeed           0.59914    0.14293   4.192 8.93e-05 ***\nmode1           0.25154    0.17838   1.410  0.16350    \nmode2          -0.17044    0.17838  -0.955  0.34305    \naltitude:speed  0.80582    0.31468   2.561  0.01289 *  \naltitude:mode1 -0.08552    0.22482  -0.380  0.70493    \naltitude:mode2  0.08892    0.22482   0.396  0.69382    \nspeed:mode1    -0.06349    0.19258  -0.330  0.74276    \nspeed:mode2     0.10190    0.19258   0.529  0.59861    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.053 on 62 degrees of freedom\nMultiple R-squared:  0.2724,    Adjusted R-squared:  0.1668 \nF-statistic: 2.579 on 9 and 62 DF,  p-value: 0.01364\n\n\nAfter analyzing the data, we see all the terms were indeed marked as significant! To see if this was just a fluke, let’s repeat the process like we did before with new sets of noise.\n\noptions(contrasts = rep(\"contr.sum\", 2))   \nlm(formula =  NIIRS ~ (altitude + speed + mode)^2,\n             data = simulated_results_design) |> \n    summary() |> \n  coef() |> \n  rownames() ->\nmodel_terms \n\nis_significant = rep(0,length(model_terms))\nnames(is_significant) = model_terms\n\nfor(i in 1:10000) {\n  set.seed(i) \n  #Create fake NIIRS results (where altitude and speed and altitude:speed are active effects)\n  simulated_results_design$NIIRS = rnorm(n = nrow(design_90runs), sd = 1, mean = 4) + \n    0.5 * simulated_results_design$altitude + \n    0.5 * simulated_results_design$speed +\n    0.5 * (simulated_results_design$speed * simulated_results_design$altitude)\n  \n  lm(formula =  NIIRS ~ (altitude + speed + mode)^2,\n             data = simulated_results_design) |> \n    summary() |> \n    coef() ->\n  fit_coefficients \n  pvals = fit_coefficients[,4] #p-values\n  is_significant = is_significant + ifelse(pvals < 0.2, 1, 0) \n}\nis_significant/10000\n\n   (Intercept)       altitude          speed          mode1          mode2 \n        1.0000         0.7069         0.9917         0.2039         0.1994 \naltitude:speed altitude:mode1 altitude:mode2    speed:mode1    speed:mode2 \n        0.6493         0.1982         0.1983         0.1940         0.2033 \n\neval_design(design_90runs, effectsize = 1, alpha=0.2)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.9883613\n2        altitude    effect.power 0.7050435\n3           speed    effect.power 0.9913139\n4            mode    effect.power 0.9622582\n5  altitude:speed    effect.power 0.6500264\n6   altitude:mode    effect.power 0.8670089\n7      speed:mode    effect.power 0.9380883\n8     (Intercept) parameter.power 0.9883613\n9        altitude parameter.power 0.7050435\n10          speed parameter.power 0.9913139\n11          mode1 parameter.power 0.9506650\n12          mode2 parameter.power 0.9506650\n13 altitude:speed parameter.power 0.6500264\n14 altitude:mode1 parameter.power 0.8522629\n15 altitude:mode2 parameter.power 0.8522629\n16    speed:mode1 parameter.power 0.9243885\n17    speed:mode2 parameter.power 0.9243885\n==============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\nNote the our simulation produced values for altitude, speed, and altitude:speed that are almost identical to the parametric power values calculated with eval_design()! And that’s because, with this simulation, power is exactly what we’ve computed; we know an effect exists and we have computed the probability that we are able to detect it with our design and analysis methods, which is the definition of statistical power. We did this with what’s called a Monte Carlo technique: we used repeated random sampling to calculate our result, rather than developing an analytic solution for power. What’s really powerful about this method is how closely linked it is with the actual analysis techniques: we are using the exact same functions to generate power estimates that we plan on using during our actual analysis. This means we have a strong justification for these numbers being accurate, and we didn’t need to make any approximations or simplifying assumptions along the way.\nA Monte Carlo power interface has been implemented in skpr via the function eval_design_mc(): it automates the above process and automatically transforms your design into the correct format, runs the simulation using the specified effect size, and then fits the results depending on what type of analysis you intend on performing. Let’s check out the slides and then dive in.\n\n\nMonte Carlo power evaluation\neval_design_mc() has an identical interface to eval_design(), but with additional options to support far more types of responses, designs, and analytic methods. Let’s run through an example, first starting with an identical model to eval_design().\n\neval_design(design = design_90runs,\n            model = ~ (altitude + speed + mode)^2,\n            alpha = 0.2,\n            effectsize = 1)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.9883613\n2        altitude    effect.power 0.7050435\n3           speed    effect.power 0.9913139\n4            mode    effect.power 0.9622582\n5  altitude:speed    effect.power 0.6500264\n6   altitude:mode    effect.power 0.8670089\n7      speed:mode    effect.power 0.9380883\n8     (Intercept) parameter.power 0.9883613\n9        altitude parameter.power 0.7050435\n10          speed parameter.power 0.9913139\n11          mode1 parameter.power 0.9506650\n12          mode2 parameter.power 0.9506650\n13 altitude:speed parameter.power 0.6500264\n14 altitude:mode1 parameter.power 0.8522629\n15 altitude:mode2 parameter.power 0.8522629\n16    speed:mode1 parameter.power 0.9243885\n17    speed:mode2 parameter.power 0.9243885\n==============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\neval_design_mc(design = design_90runs,\n               model = ~ (altitude + speed + mode)^2,\n               alpha = 0.2,\n               effectsize = 1,\n               glmfamily = \"gaussian\",\n               nsim = 100)\n\n        parameter               type power\n1     (Intercept)    effect.power.mc  0.98\n2        altitude    effect.power.mc  0.74\n3           speed    effect.power.mc  0.99\n4            mode    effect.power.mc  0.95\n5  altitude:speed    effect.power.mc  0.71\n6   altitude:mode    effect.power.mc  0.82\n7      speed:mode    effect.power.mc  0.91\n8     (Intercept) parameter.power.mc  0.98\n9        altitude parameter.power.mc  0.74\n10          speed parameter.power.mc  0.99\n11          mode1 parameter.power.mc  0.93\n12          mode2 parameter.power.mc  0.95\n13 altitude:speed parameter.power.mc  0.71\n14 altitude:mode1 parameter.power.mc  0.79\n15 altitude:mode2 parameter.power.mc  0.81\n16    speed:mode1 parameter.power.mc  0.89\n17    speed:mode2 parameter.power.mc  0.95\n=============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\nSince this is a Monte Carlo estimate of power, there is sampling error of magnitude 1/sqrt(100) = 0.1 associated with these estimates. To decrease this error, we can simply increase the number of simulations. Increasing nsim to 1000 will lower the magnitude of the error to 0.03.\n\neval_design_mc(design = design_90runs,\n               model = ~ (altitude + speed + mode)^2,\n               alpha = 0.2,\n               effectsize = 1,\n               glmfamily = \"gaussian\",\n               nsim = 1000)\n\n        parameter               type power\n1     (Intercept)    effect.power.mc 0.990\n2        altitude    effect.power.mc 0.715\n3           speed    effect.power.mc 0.998\n4            mode    effect.power.mc 0.971\n5  altitude:speed    effect.power.mc 0.667\n6   altitude:mode    effect.power.mc 0.858\n7      speed:mode    effect.power.mc 0.930\n8     (Intercept) parameter.power.mc 0.990\n9        altitude parameter.power.mc 0.715\n10          speed parameter.power.mc 0.998\n11          mode1 parameter.power.mc 0.965\n12          mode2 parameter.power.mc 0.953\n13 altitude:speed parameter.power.mc 0.667\n14 altitude:mode1 parameter.power.mc 0.851\n15 altitude:mode2 parameter.power.mc 0.854\n16    speed:mode1 parameter.power.mc 0.927\n17    speed:mode2 parameter.power.mc 0.913\n=============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode \n* Anticipated Coefficients = c(0.500, 0.500, 0.500, 0.500, -0.500, 0.500, 0.500, -0.500, 0.500, -0.500) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\nThe primary utility for eval_design_mc() isn’t with normal responses, however.\nIf our response variable is binomial and we are evaluating a probability (such as the probability of correctly classifying an aircraft in our SAR image), we can use a generalized linear model to perform a logistic regression to estimate those probabilities directly. To estimate power for this model, we simply change glmfamily to \"binomial\" in eval_design_mc(), and specify our effectsize as a low and high probability. We’ll say here we are looking to detect when the probability of correctly classifying an aircraft in a SAR image changes from 0.50 to 0.90. There’s no closed-form solution for calculating this, but there are some approximate methods available. One common method is to calculate an approximate signal-to-noise ratio for the two probabilities and then perform a traditional power calculation using that SNR. How well does this work?\n\n#Calculate SNR approximation using \"logit\" method\nhigh_prob = 0.9\nlow_prob = 0.5\naverage_prob = (high_prob + low_prob)/2\n\ndelta = abs(log(low_prob/(1-low_prob)) - log(high_prob/(1-high_prob)))\nnoise = sqrt(average_prob/(1-average_prob))\nsnr_approximation = delta/noise\n\neval_design(design = design_90runs,\n             model = ~ (altitude + speed + mode)^2,\n             alpha = 0.2,\n             effectsize = snr_approximation)\n\n        parameter            type     power\n1     (Intercept)    effect.power 0.9999347\n2        altitude    effect.power 0.9087872\n3           speed    effect.power 0.9999660\n4            mode    effect.power 0.9993413\n5  altitude:speed    effect.power 0.8665973\n6   altitude:mode    effect.power 0.9873736\n7      speed:mode    effect.power 0.9979037\n8     (Intercept) parameter.power 0.9999347\n9        altitude parameter.power 0.9087872\n10          speed parameter.power 0.9999660\n11          mode1 parameter.power 0.9983423\n12          mode2 parameter.power 0.9983423\n13 altitude:speed parameter.power 0.8665973\n14 altitude:mode1 parameter.power 0.9805725\n15 altitude:mode2 parameter.power 0.9805725\n16    speed:mode1 parameter.power 0.9956789\n17    speed:mode2 parameter.power 0.9956789\n==============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode \n* Anticipated Coefficients = c(0.719, 0.719, 0.719, 0.719, -0.719, 0.719, 0.719, -0.719, 0.719, -0.719) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\neval_design_mc(design = design_90runs,\n               model = ~ (altitude + speed + mode)^2,\n               alpha = 0.2,\n               effectsize = c(0.5,0.9),\n               glmfamily = \"binomial\", \n               nsim = 1000)\n\nWarning in eval_design_mc(design = design_90runs, model = ~(altitude + speed +\n: skpr: Partial or complete separation likely detected in the binomial Monte\nCarlo simulation. Increase the number of runs in the design or decrease the\nnumber of model parameters to improve power.\n\n\n        parameter               type power\n1     (Intercept)    effect.power.mc 0.172\n2        altitude    effect.power.mc 0.104\n3           speed    effect.power.mc 0.175\n4            mode    effect.power.mc 0.340\n5  altitude:speed    effect.power.mc 0.349\n6   altitude:mode    effect.power.mc 0.305\n7      speed:mode    effect.power.mc 0.265\n8     (Intercept) parameter.power.mc 0.172\n9        altitude parameter.power.mc 0.104\n10          speed parameter.power.mc 0.175\n11          mode1 parameter.power.mc 0.121\n12          mode2 parameter.power.mc 0.172\n13 altitude:speed parameter.power.mc 0.349\n14 altitude:mode1 parameter.power.mc 0.166\n15 altitude:mode2 parameter.power.mc 0.165\n16    speed:mode1 parameter.power.mc 0.097\n17    speed:mode2 parameter.power.mc 0.146\n=============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode \n* Anticipated Coefficients = c(1.099, 1.099, 1.099, 1.099, -1.099, 1.099, 1.099, -1.099, 1.099, -1.099) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = glm(..., family = \"binomial\")` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\n\nThe calculated power values don’t match. Even worse, our power values are falling below our acceptable Type-I error rate of 0.2, which suggests a more fundamental issue is at play. A warning message after eval_design_mc() gives us a clue why: “Partial or complete separation likely detected in the binomial Monte Carlo simulation. Increase the number of runs in the design or decrease the number of model parameters to improve power.” Separation is an issue you can encounter in logistic regression when your model perfectly predicts the outcome for some combination of parameters and does not converge. The approximate method does not capture problems like this: you would only know about this issue if you ran a simulation ahead of time. And since we know about it, we can : let’s add a Firth correction to the logistic regression by setting firth = TRUE, which removes the issue of separation and allows the model to converge and give us useful power estimates. Since the Firth correction is more computationally expensive, we’ll also turn on parallel processing to speed up the computation.\n\neval_design_mc(design = design_90runs,\n               model = ~ (altitude + speed + mode)^2,\n               alpha = 0.2,\n               effectsize = c(0.5,0.9),\n               glmfamily = \"binomial\", \n               firth = TRUE,\n               nsim = 1000, \n               parallel=TRUE)\n\n        parameter               type power\n1        altitude    effect.power.mc 0.926\n2           speed    effect.power.mc 0.988\n3            mode    effect.power.mc 0.964\n4  altitude:speed    effect.power.mc 0.558\n5   altitude:mode    effect.power.mc 0.887\n6      speed:mode    effect.power.mc 0.754\n7     (Intercept) parameter.power.mc 0.899\n8        altitude parameter.power.mc 0.857\n9           speed parameter.power.mc 0.806\n10          mode1 parameter.power.mc 0.675\n11          mode2 parameter.power.mc 0.732\n12 altitude:speed parameter.power.mc 0.733\n13 altitude:mode1 parameter.power.mc 0.794\n14 altitude:mode2 parameter.power.mc 0.753\n15    speed:mode1 parameter.power.mc 0.742\n16    speed:mode2 parameter.power.mc 0.474\n=============Evaluation Info==============\n* Alpha = 0.2 * Trials = 72 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode + altitude:speed + altitude:mode + speed:mode \n* Anticipated Coefficients = c(1.099, 1.099, 1.099, 1.099, -1.099, 1.099, 1.099, -1.099, 1.099, -1.099) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = glm(..., family = \"binomial\", method = mbest::firthglm.fit)` \n* Effect Analysis Method = lmtest::lrtest(fit, fit_without_effect)` \n\n\nNow we’ve improved our power substantially, but note that it still isn’t close to the extremely optimistic power values given using the approximate method.\n\n\nChecking for Type-I error inflation\nLet’s say we get tasked to find the absolute bare-minimum test we could execute and still find adequate. As part of that tasking, you start playing with various design sizes, removing model terms, increasing effect sizes, and changing analysis methods to try and achieve 80% power. You discover you need at least 6 runs to fit any model, so you take that design and discover something interesting when you switch from a Type-III ANOVA to a likelihood ratio test for power evaluation:\n\nset.seed(123)\ngen_design(constrained_candidate_set, ~altitude + speed + mode, trials=6) |> \n  eval_design_mc(model = ~ altitude + speed + mode,\n                 alpha=0.2, \n                 effect_anova = TRUE, \n                 effectsize = 2)\n\n    parameter               type power\n1 (Intercept)    effect.power.mc 0.517\n2    altitude    effect.power.mc 0.429\n3       speed    effect.power.mc 0.428\n4        mode    effect.power.mc 0.338\n5 (Intercept) parameter.power.mc 0.517\n6    altitude parameter.power.mc 0.429\n7       speed parameter.power.mc 0.428\n8       mode1 parameter.power.mc 0.398\n9       mode2 parameter.power.mc 0.410\n===========Evaluation Info============\n* Alpha = 0.2 * Trials = 6 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode \n* Anticipated Coefficients = c(1, 1, 1, 1, -1) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\nset.seed(123)\n#Now calculate effect power with a likelihood ratio test\ngen_design(constrained_candidate_set, ~altitude + speed + mode, trials=6) |> \n  eval_design_mc(model = ~ altitude + speed + mode,\n                 alpha=0.2, \n                 effect_anova = FALSE, \n                 effectsize = 2)\n\n    parameter               type power\n1    altitude    effect.power.mc 0.929\n2       speed    effect.power.mc 0.924\n3        mode    effect.power.mc 0.921\n4 (Intercept) parameter.power.mc 0.517\n5    altitude parameter.power.mc 0.429\n6       speed parameter.power.mc 0.428\n7       mode1 parameter.power.mc 0.398\n8       mode2 parameter.power.mc 0.410\n===========Evaluation Info============\n* Alpha = 0.2 * Trials = 6 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode \n* Anticipated Coefficients = c(1, 1, 1, 1, -1) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `lmtest::lrtest(fit, fit_without_effect)` \n\n\nHere, we see the likelihood ratio test is giving effect power values above 90% at only 6 runs! I did mention earlier that some analytic methods are more powerful than others: is that what’s going on here? To find out, let’s do a sanity check on our Type-I error rate. As I mentioned earlier, power depends on several inputs. One of them is your Type-I error rate, which is set by the tester ahead of time. Or is it? Let’s check the Type-I error rate by setting effectsize = 0: we’re testing the power when the null hypothesis is true. We’ll do this for both the Type-III ANOVA effect power and the likelihood ratio test power calculations.\n\nset.seed(123)\ngen_design(constrained_candidate_set, ~altitude + speed + mode, trials=6) |> \n  eval_design_mc(model = ~ altitude + speed + mode,\n                 alpha=0.2, \n                 effect_anova = TRUE, \n                 effectsize = 0)\n\n    parameter               type power\n1 (Intercept)    effect.power.mc 0.204\n2    altitude    effect.power.mc 0.199\n3       speed    effect.power.mc 0.194\n4        mode    effect.power.mc 0.197\n5 (Intercept) parameter.power.mc 0.204\n6    altitude parameter.power.mc 0.199\n7       speed parameter.power.mc 0.194\n8       mode1 parameter.power.mc 0.200\n9       mode2 parameter.power.mc 0.200\n===========Evaluation Info============\n* Alpha = 0.2 * Trials = 6 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode \n* Anticipated Coefficients = c(0, 0, 0, 0, -0) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `car::Anova(fit, type = \"III\")` \n\nset.seed(123)\ngen_design(constrained_candidate_set, ~altitude + speed + mode, trials=6) |> \n  eval_design_mc(model = ~ altitude + speed + mode,\n                 alpha=0.2, \n                 effect_anova = FALSE, \n                 effectsize = 0)\n\n    parameter               type power\n1    altitude    effect.power.mc 0.638\n2       speed    effect.power.mc 0.672\n3        mode    effect.power.mc 0.760\n4 (Intercept) parameter.power.mc 0.204\n5    altitude parameter.power.mc 0.199\n6       speed parameter.power.mc 0.194\n7       mode1 parameter.power.mc 0.200\n8       mode2 parameter.power.mc 0.200\n===========Evaluation Info============\n* Alpha = 0.2 * Trials = 6 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode \n* Anticipated Coefficients = c(0, 0, 0, 0, -0) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `lmtest::lrtest(fit, fit_without_effect)` \n\n\nOur Type-I error rate for the likelihood ratio test is close to 75%! That means that we are incorrectly calling the system effective 75% of the time–a terrible outcome. Let’s see what’s going on by plotting Type-I error as a function of sample size. We can use eval_design_mc() in calculate_power_curves() simply by specifying eval_function = \"eval_design_mc\" and the arguments in eval_args.\n\npower_values = calculate_power_curves(trials = seq(6,30,by=2),\n                       candidateset = constrained_candidate_set,\n                       model = ~ altitude + speed + mode,\n                       alpha = 0.2,\n                       effectsize = 0, \n                       eval_function = \"eval_design_mc\",\n                       x_breaks =  seq(6,30,by=2),\n                       eval_args = list(nsim=1000, effect_anova = FALSE),\n                       ggplot_elements = list(geom_hline(yintercept = 0.8, \n                                                         alpha = 0.5, \n                                                         linetype = \"dashed\", \n                                                         color = \"red\", \n                                                         linewidth = 1)))\n\n\n\n\nWe see at low sample sizes, we our Type-I error rate is severely inflated with the likelihood ratio test. This is because the likelihood ratio test is based on an approximation of the log-likelihood to a chi-squared distribution, and that approximation fails at low sample sizes. We can correct that in skpr by setting adjust_alpha_inflation = TRUE, which runs the simulation twice: first to calculate the empirical distribution of p-values under the null hypothesis and find the true Type-I error cutoff that corresponds to your desired Type-I error rate, and then again with the actual effect size input by the user. Let’s see how that works:\n\nset.seed(123)\ngen_design(constrained_candidate_set, ~altitude + speed + mode, trials=6) |> \n  eval_design_mc(model = ~ altitude + speed + mode,\n                 alpha=0.2, \n                 effect_anova = FALSE, adjust_alpha_inflation = TRUE,\n                 effectsize = 0)\n\n    parameter               type power\n1    altitude    effect.power.mc 0.199\n2       speed    effect.power.mc 0.203\n3        mode    effect.power.mc 0.195\n4 (Intercept) parameter.power.mc 0.199\n5    altitude parameter.power.mc 0.194\n6       speed parameter.power.mc 0.200\n7       mode1 parameter.power.mc 0.203\n8       mode2 parameter.power.mc 0.197\n===========Evaluation Info============\n* Alpha = 0.2 * Trials = 6 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode \n* Anticipated Coefficients = c(0, 0, 0, 0, -0) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `lmtest::lrtest(fit, fit_without_effect)` \n\nset.seed(123)\ngen_design(constrained_candidate_set, ~altitude + speed + mode, trials=6) |> \n  eval_design_mc(model = ~ altitude + speed + mode,\n                 alpha=0.2, \n                 effect_anova = FALSE, adjust_alpha_inflation = TRUE,\n                 effectsize = 2)\n\n    parameter               type power\n1    altitude    effect.power.mc 0.443\n2       speed    effect.power.mc 0.456\n3        mode    effect.power.mc 0.329\n4 (Intercept) parameter.power.mc 0.538\n5    altitude parameter.power.mc 0.432\n6       speed parameter.power.mc 0.449\n7       mode1 parameter.power.mc 0.390\n8       mode2 parameter.power.mc 0.390\n===========Evaluation Info============\n* Alpha = 0.2 * Trials = 6 * Blocked = FALSE \n* Evaluating Model = ~altitude + speed + mode \n* Anticipated Coefficients = c(1, 1, 1, 1, -1) \n* Contrasts = `contr.sum` \n* Parameter Analysis Method = `lm(...)` \n* Effect Analysis Method = `lmtest::lrtest(fit, fit_without_effect)` \n\nset.seed(123)\nfor(i in 1:100) {\ngen_design(constrained_candidate_set, ~altitude + speed + mode, trials=6) |> \n  eval_design_mc(model = ~ altitude + speed + mode,\n                 alpha=0.2, \n                 effect_anova = TRUE, \n                 effectsize = 2)\n}\n\nWe see when we adjust for Type-I error inflation, the power “advantage” for the likelihood ratio test disappears. Type-I error inflation occurs all the time, particularly with blocking and split-plot designs, so I recommend always checking Type-I error as part of your DOE workflow: it only takes a minute and serves as a sanity check if your numbers seem too good to be true.\n\n\nFinal analysis\nLet’s put this all together. We decide to use a model with all interactions, and also investigate two different effect sizes for an objective and a threshold requirement. We’re doing a Monte Carlo simulation with 100 simulations and a binomial response, and we want to see at what point from 30 to 360 runs we reach 80% power for all model terms. We can do this entire analysis in a single call to calculate_power_curves().\n\nset.seed(2023)\npower_values = calculate_power_curves(trials = seq(30,360,by=30),\n                       candidateset = constrained_candidate_set,\n                       model = ~ (altitude + speed + mode)^2,\n                       alpha = 0.2,\n                       effectsize = list(c(0.5,0.9), c(0.7,0.9)), \n                       eval_function = \"eval_design_mc\",\n                       eval_args = list(nsim=1000, glmfamily = \"binomial\"),\n                       x_breaks = seq(30,360,by=30),\n                       ggplot_elements = list(geom_hline(yintercept = 0.8, \n                                                         alpha = 0.5, \n                                                         linetype = \"dashed\", \n                                                         color = \"red\", \n                                                         linewidth = 1))) \n\n\n\nhead(power_values, 10)\n\n        parameter               type power trials effectsize_low\n1     (Intercept)    effect.power.mc 0.005     30            0.5\n2        altitude    effect.power.mc 0.001     30            0.5\n3           speed    effect.power.mc 0.003     30            0.5\n4            mode    effect.power.mc 0.003     30            0.5\n5  altitude:speed    effect.power.mc 0.014     30            0.5\n6   altitude:mode    effect.power.mc 0.007     30            0.5\n7      speed:mode    effect.power.mc 0.004     30            0.5\n8     (Intercept) parameter.power.mc 0.005     30            0.5\n9        altitude parameter.power.mc 0.001     30            0.5\n10          speed parameter.power.mc 0.003     30            0.5\n   effectsize_high random_seed\n1              0.9         123\n2              0.9         123\n3              0.9         123\n4              0.9         123\n5              0.9         123\n6              0.9         123\n7              0.9         123\n8              0.9         123\n9              0.9         123\n10             0.9         123\nPower curve generation captured the following warning/error messages:\nFunction   | Type | N  | Message\nEvaluation | Warn | 24 | Message: 'skpr: Partial or complete separation likely detected in the binomial Monte Carlo simulation. Increase the number of runs in the design or decrease the number of model parameters to improve power.'\n\n\nThis analysis shows that we would need around 300 runs at an effect size of 0.5 to 0.9 and if we wanted to be able to estimate all terms in the model with 80% power. At the objective requirement of 0.7 to 0.9, we don’t see some model terms crossing 80% power even at 360 runs."
  },
  {
    "objectID": "skpr_gen_design_live_demo.html",
    "href": "skpr_gen_design_live_demo.html",
    "title": "gen_design_basics",
    "section": "",
    "text": "Load our libraries\n\nlibrary(skpr)\nlibrary(tidyverse)\n\n\n\nCreating the candidate set and the constrained candidate set\nWe want to run a test on a synthetic aperture radar attached to an aircraft. We want to characterize how the resolution of the collected imagery changes depending on the speed, altitude, and mode of the radar.\nWe first want to generate our flight envelope. We do this by generating a table (data.frame) of all combinations of our input factors with expand.grid(), and then filtering out all those that don’t match our constraints.\n\n#Small example of what expand.grid() outputs\nexpand.grid(altitude    = seq(10000,30000,by=10000),\n            speed       = seq(450,550,by=50),\n            mode        = c(\"scan\", \"strip\", \"spotlight\"),\n            target_env  = c(\"urban\", \"desert\"))\n\n   altitude speed      mode target_env\n1     10000   450      scan      urban\n2     20000   450      scan      urban\n3     30000   450      scan      urban\n4     10000   500      scan      urban\n5     20000   500      scan      urban\n6     30000   500      scan      urban\n7     10000   550      scan      urban\n8     20000   550      scan      urban\n9     30000   550      scan      urban\n10    10000   450     strip      urban\n11    20000   450     strip      urban\n12    30000   450     strip      urban\n13    10000   500     strip      urban\n14    20000   500     strip      urban\n15    30000   500     strip      urban\n16    10000   550     strip      urban\n17    20000   550     strip      urban\n18    30000   550     strip      urban\n19    10000   450 spotlight      urban\n20    20000   450 spotlight      urban\n21    30000   450 spotlight      urban\n22    10000   500 spotlight      urban\n23    20000   500 spotlight      urban\n24    30000   500 spotlight      urban\n25    10000   550 spotlight      urban\n26    20000   550 spotlight      urban\n27    30000   550 spotlight      urban\n28    10000   450      scan     desert\n29    20000   450      scan     desert\n30    30000   450      scan     desert\n31    10000   500      scan     desert\n32    20000   500      scan     desert\n33    30000   500      scan     desert\n34    10000   550      scan     desert\n35    20000   550      scan     desert\n36    30000   550      scan     desert\n37    10000   450     strip     desert\n38    20000   450     strip     desert\n39    30000   450     strip     desert\n40    10000   500     strip     desert\n41    20000   500     strip     desert\n42    30000   500     strip     desert\n43    10000   550     strip     desert\n44    20000   550     strip     desert\n45    30000   550     strip     desert\n46    10000   450 spotlight     desert\n47    20000   450 spotlight     desert\n48    30000   450 spotlight     desert\n49    10000   500 spotlight     desert\n50    20000   500 spotlight     desert\n51    30000   500 spotlight     desert\n52    10000   550 spotlight     desert\n53    20000   550 spotlight     desert\n54    30000   550 spotlight     desert\n\ncandidate_set = expand.grid(altitude    = seq(10000,30000,by=1000),\n                            speed       = seq(450,550,by=5),\n                            mode        = c(\"scan\", \"strip\", \"spotlight\"),\n                            target_env  = c(\"urban\", \"desert\"))\nnrow(candidate_set)\n\n[1] 2646\n\n\n\n\nGenerating our first design with the unconstrained candidate set to see the classical design\nFirst we’ll generate a design using the full linear model with all interactions. Our first test design will have 24 runs.\n\nset.seed(2023)\nflight_design_no_constraints = gen_design(candidateset = candidate_set,\n                           model = ~(altitude + speed + mode)^2,\n                           trials = 24,\n                           randomized = FALSE) \nflight_design_no_constraints\n\n   altitude speed      mode\n1     10000   450      scan\n2     10000   450      scan\n3     10000   450     strip\n4     10000   450     strip\n5     10000   450 spotlight\n6     10000   450 spotlight\n7     10000   550      scan\n8     10000   550      scan\n9     10000   550     strip\n10    10000   550     strip\n11    10000   550 spotlight\n12    10000   550 spotlight\n13    30000   450      scan\n14    30000   450      scan\n15    30000   450     strip\n16    30000   450     strip\n17    30000   450 spotlight\n18    30000   450 spotlight\n19    30000   550      scan\n20    30000   550      scan\n21    30000   550     strip\n22    30000   550     strip\n23    30000   550 spotlight\n24    30000   550 spotlight\n\nget_optimality(flight_design_no_constraints)\n\n    D        I   A            G   T  E Alias\n1 100 0.212963 100 Not Computed 240 24     9\n\nggplot(flight_design_no_constraints) +\n  geom_point(data=candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n\nNote that randomization doesn’t affect the design–it simply rearranges it so it’s easier to read. The order of the design doesn’t affect the design metrics in any way. However, the tester should randomize the runs when the test is executed.\nThis generated the classical experimental design for this candidate set: test points at the corners of the space.\n\n\nDesign generation with a constrained candidate set\nNow let’s see what design points are selected when we constrain our candidate set to the allowable flight envelope.\n\nconstrained_candidate_set = candidate_set |> \n  filter(70000 - speed * 90 > altitude) |> \n  filter(-430000 + speed * 1000 > altitude) |> \n  filter(150000 - speed * 300 < altitude)  |> \n  filter(-14000 + speed * 50 < altitude)\n\n#Plot the new constrained candidate set\nggplot(data=constrained_candidate_set) +\n  geom_point(aes(x=speed,y=altitude))  +\n  geom_abline(slope = -90, intercept=70000,color=\"red\") +\n  geom_abline(slope = 1000, intercept=-430000,color=\"red\") +\n  geom_abline(slope = -300, intercept=150000,color=\"red\") +\n  geom_abline(slope = 50, intercept=-14000,color=\"red\") +\n  labs(title = \"Testing Flight Envelope\") +\n  facet_wrap(~mode)\n\n\n\nset.seed(2023)\nflight_design = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ (altitude + speed + mode)^2,\n                           trials = 24,\n                           randomized=FALSE) \nflight_design\n\n   altitude speed      mode\n1     10000   470      scan\n2     10000   470      scan\n3     10000   470     strip\n4     10000   470     strip\n5     10000   470 spotlight\n6     10000   470 spotlight\n7     14000   550      scan\n8     14000   550      scan\n9     14000   550     strip\n10    14000   550     strip\n11    14000   550 spotlight\n12    14000   550 spotlight\n13    20000   550      scan\n14    20000   550      scan\n15    20000   550     strip\n16    20000   550     strip\n17    20000   550 spotlight\n18    20000   550 spotlight\n19    28000   460      scan\n20    28000   460      scan\n21    28000   460     strip\n22    28000   460     strip\n23    28000   460 spotlight\n24    28000   460 spotlight\n\nget_optimality(flight_design)\n\n         D         I        A            G        T        E Alias\n1 62.33936 0.3402056 44.99871 Not Computed 175.7037 2.453013     9\n\nggplot(flight_design) +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n\nWe see we are now selecting the “corners” of our flight envelope.\n\n\nGenerating designs that can detect curvature\nLet’s say we suspect that the behavior of the NIIRS rating may not be linear–for example, we might suspect that at low altitudes we may have poor resolution due to shadowing effects, while at high altitudes the distance may negatively impact resolution, and we want to detect the sweet spot in-between. This means we want to be able to detect “curvature,” which we specify in an R formula with the “as.is” function, I(). This function evaluates the expression contained within rather than interpreting it as part of the formula. For example, the formula expression ~(x+y)^2 is interpreted by R as fitting a main effects model with interactions between x and y ~x + y + x:y, while ~I((x+y)^2) is interpreted as fitting a new column of value (x+y)^2. This function is just shorthand for adding a new column to your data.frame (e.g. design$x_squared = design$x * design$x) with the arithmetic value computed within.\n\nset.seed(2023)\nflight_design_quadratic = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n                           trials = 24) \nget_optimality(flight_design_quadratic)\n\n         D         I       A            G        T         E Alias\n1 42.11373 0.5627597 18.5322 Not Computed 187.1708 0.7093834     9\n\nggplot(flight_design_quadratic) +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n\nWe see we now have center points long with points on the extrema: these points allow us to determine if the relationship is linear or has curvature.\n\n\nFilling the design space\nHowever, you might note that This appears to be only partially filling up the design space–one indication that your design could be improved is if there is a lack of balance. Let’s increase the number of runs and see where the additional test points end up.\n\nset.seed(2023)\nflight_design_quadratic2 = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n                           trials = 72) \n\n\nggplot(flight_design_quadratic2) +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n\nNow the three factors are fairly well balanced. When a design is well-balanced and adding more points results only in replicating additional test points, you can at least be assured that you are covering the space well (although those replicates might be required to have sufficient power to answer your hypothesis, depending on your effect size).\nHowever, we should only expect balance between factors when the design space is balanced: disallowed combinations can actually make it so the optimal design also isn’t balanced. We can add a constraint to one of the modes to see how it results in the design losing symmetry across modes.\n\nset.seed(2023)\nconstrained_candidate_set2 = filter(constrained_candidate_set, mode != \"spotlight\" | altitude < 25000)\nflight_design_quadratic3 = gen_design(candidateset = constrained_candidate_set2, \n             model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n             trials = 72) \n\nggplot(flight_design_quadratic3) +\n  geom_point(data=constrained_candidate_set2, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n\n\n\nI-Optimality\nLet’s say we’re more interested in predicting how the NIIRS value varies across the space, rather than just estimating the size of the effects from each model term. Here, we will want to use something called an I-optimal design, which minimizes the average prediction variance across the space.\n\nset.seed(2023)\nflight_design_prediction = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n                           trials = 72,\n                           optimality = \"I\") \nget_optimality(flight_design_prediction)\n\n         D         I        A            G        T        E Alias\n1 39.75236 0.1515717 24.90418 Not Computed 476.1945 3.999816     9\n\n#Compare it to the previous design\nget_optimality(flight_design_quadratic2)\n\n         D         I        A            G        T        E Alias\n1 42.82161 0.1747125 20.54451 Not Computed 554.1562 2.570192     9\n\n#D-optimal design\nggplot(flight_design_quadratic2) +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n#I-optimal design\nggplot(flight_design_prediction) +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n\nHowever, these optimality criteria don’t tell us much, other than we found a design that’s better in one criteria than the other. A design can be mathematically optimal for a given design criterion and specified number of runs but still be inadequate for the actual experimental goal(s). We need other ways to evaluate the designs to decide which one to choose.\n\n\nComparing D and I optimal designs with Fraction of Design Space plots\nIdeally, a design would have low prediction variance across the entire design space. We can use a visualization called a Fraction of Design Space (FDS) plot to help choose between designs if this is the primary metric we are interested in. A FDS Plot shows the proportion of the design space over which the relative prediction variance lies below a given value. For a hypothetical ideal design, this graph would be a flat horizontal line: this means you would have the same prediction variance everywhere. Realistically, the prediction variance will always vary across the design space, as we only have a finite number of test points to allocate. When comparing designs using FDS plots we can assess designs by how much of the design space is under a particular prediction variance value as well as what the worst case scenario for prediction variance is. I-optimal designs are specifically designed for prediction, so let’s see how an I-optimal design compares to a D-optimal using an FDS plot.\n\nplot_fds(flight_design_quadratic2, yaxis_max = 1, description = \"Fraction of Design Space - D-optimal\")\n\n\n\nget_optimality(flight_design_quadratic2, \"I\")\n\n          I\n1 0.1747125\n\nplot_fds(flight_design_prediction, yaxis_max = 1, description = \"Fraction of Design Space - I-optimal\")\n\n\n\nget_optimality(flight_design_prediction, \"I\")\n\n          I\n1 0.1515717\n\n\nWe see that the I-optimal design has a lower overall average prediction variance across most of the space, but also has areas with much higher prediction variance. Depending on your experimental goals, you might value one property or the other.\n\n\nSplit-plot designs\nSee slides.\nWe also have included a hard-to-change factor Target Environment in our candidate set. We can construct a split-plot design in layers: first by building a design for the hard-to-change factors, fixing those blocks, and then building a design for just the easy-to-change factors.\n\nset.seed(2023)\nflight_design_htc = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ target_env,\n                           trials = 8) \nflight_design_htc\n\n  target_env\n1      urban\n2     desert\n3      urban\n4      urban\n5     desert\n6      urban\n7     desert\n8     desert\n\nflight_design_splitplot = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ (altitude + speed + mode + target_env)^2 + I(speed^2) + I(altitude^2),\n                           splitplotdesign = flight_design_htc,\n                           trials = 64,\n                           parallel = TRUE, add_blocking_columns = TRUE) \nflight_design_splitplot\n\n    Block1 target_env altitude speed      mode\n1.1      1      urban    28000   460     strip\n1.2      1      urban    19000   500      scan\n1.3      1      urban    10000   470     strip\n1.4      1      urban    28000   460 spotlight\n1.5      1      urban    20000   550 spotlight\n1.6      1      urban    10000   470 spotlight\n1.7      1      urban    14000   550     strip\n1.8      1      urban    16000   450      scan\n2.1      2     desert    14000   550     strip\n2.2      2     desert    24000   510      scan\n2.3      2     desert    28000   460 spotlight\n2.4      2     desert    10000   470 spotlight\n2.5      2     desert    19000   450     strip\n2.6      2     desert    10000   470      scan\n2.7      2     desert    28000   460     strip\n2.8      2     desert    20000   550 spotlight\n3.1      3      urban    10000   470     strip\n3.2      3      urban    14000   550 spotlight\n3.3      3      urban    20000   550      scan\n3.4      3      urban    28000   460 spotlight\n3.5      3      urban    24000   510     strip\n3.6      3      urban    10000   470      scan\n3.7      3      urban    19000   450 spotlight\n3.8      3      urban    28000   460      scan\n4.1      4      urban    19000   450     strip\n4.2      4      urban    20000   550      scan\n4.3      4      urban    10000   470 spotlight\n4.4      4      urban    14000   550 spotlight\n4.5      4      urban    14000   550     strip\n4.6      4      urban    10000   470      scan\n4.7      4      urban    28000   460      scan\n4.8      4      urban    24000   510 spotlight\n5.1      5     desert    14000   550 spotlight\n5.2      5     desert    28000   460 spotlight\n5.3      5     desert    10000   470 spotlight\n5.4      5     desert    24000   510     strip\n5.5      5     desert    20000   550      scan\n5.6      5     desert    10000   470     strip\n5.7      5     desert    18000   450      scan\n5.8      5     desert    12000   515     strip\n6.1      6      urban    28000   460     strip\n6.2      6      urban    14000   550      scan\n6.3      6      urban    28000   460      scan\n6.4      6      urban    20000   550     strip\n6.5      6      urban    20000   550 spotlight\n6.6      6      urban    16000   450 spotlight\n6.7      6      urban    10000   470     strip\n6.8      6      urban    12000   515      scan\n7.1      7     desert    28000   460      scan\n7.2      7     desert    16000   450     strip\n7.3      7     desert    20000   550     strip\n7.4      7     desert    10000   470      scan\n7.5      7     desert    28000   460 spotlight\n7.6      7     desert    14000   550      scan\n7.7      7     desert    19000   500 spotlight\n7.8      7     desert    12000   515 spotlight\n8.1      8     desert    16000   450 spotlight\n8.2      8     desert    10000   470     strip\n8.3      8     desert    20000   550     strip\n8.4      8     desert    28000   460      scan\n8.5      8     desert    14000   550      scan\n8.6      8     desert    20000   550 spotlight\n8.7      8     desert    28000   460     strip\n8.8      8     desert    18000   495      scan\n\nggplot(flight_design_splitplot) +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(target_env~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n\nThe blocking information is by default stored in the row names, but you can turn add_blocking_columns = TRUE and blocking columns will be added to the design. skpr knows to look for these and will handle them separately from factor terms.\nYou can repeat this process any number of times for nested split-plot designs where you have various blocking levels that have different levels (e.g. split-split plot, split-split-split plot, etc). We can also manually specify the split-plot sizes for each whole plot: if you have a test constraint where you know that some split plot levels are going to have more runs that others, you can account for that here.\n\nflight_design_splitplot_unbalanced = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ (altitude + speed + mode + target_env)^2 + I(speed^2) + I(altitude^2),\n                           splitplotdesign = flight_design_htc,\n                           trials = 64, blocksizes = c(4,4,12,12,4,4,12,12),\n                           parallel = TRUE, add_blocking_columns = TRUE) \nflight_design_splitplot_unbalanced\n\n     Block1 target_env altitude speed      mode\n1.1       1      urban    18000   495      scan\n1.2       1      urban    28000   460     strip\n1.3       1      urban    14000   550     strip\n1.4       1      urban    10000   470 spotlight\n2.1       2     desert    20000   550     strip\n2.2       2     desert    12000   515 spotlight\n2.3       2     desert    16000   450      scan\n2.4       2     desert    28000   460 spotlight\n3.1       3      urban    28000   460      scan\n3.2       3      urban    19000   495     strip\n3.3       3      urban    14000   550 spotlight\n3.4       3      urban    10000   470     strip\n3.5       3      urban    28000   460 spotlight\n3.6       3      urban    20000   550     strip\n3.7       3      urban    16000   450      scan\n3.8       3      urban    14000   550      scan\n3.9       3      urban    28000   460     strip\n3.10      3      urban    20000   550      scan\n3.11      3      urban    10000   470 spotlight\n3.12      3      urban    24000   510 spotlight\n4.1       4      urban    10000   470      scan\n4.2       4      urban    14000   550 spotlight\n4.3       4      urban    28000   460      scan\n4.4       4      urban    20000   550     strip\n4.5       4      urban    28000   460 spotlight\n4.6       4      urban    10000   470      scan\n4.7       4      urban    16000   450     strip\n4.8       4      urban    10000   470     strip\n4.9       4      urban    19000   500 spotlight\n4.10      4      urban    28000   460     strip\n4.11      4      urban    18000   450 spotlight\n4.12      4      urban    20000   550      scan\n5.1       5     desert    28000   460      scan\n5.2       5     desert    24000   510     strip\n5.3       5     desert    16000   450     strip\n5.4       5     desert    14000   550      scan\n6.1       6      urban    28000   460      scan\n6.2       6      urban    20000   550 spotlight\n6.3       6      urban    16000   450 spotlight\n6.4       6      urban    12000   515     strip\n7.1       7     desert    19000   450     strip\n7.2       7     desert    28000   460 spotlight\n7.3       7     desert    20000   550 spotlight\n7.4       7     desert    28000   460     strip\n7.5       7     desert    19000   450      scan\n7.6       7     desert    10000   470     strip\n7.7       7     desert    10000   470 spotlight\n7.8       7     desert    20000   550      scan\n7.9       7     desert    14000   550     strip\n7.10      7     desert    24000   510      scan\n7.11      7     desert    12000   515 spotlight\n7.12      7     desert    10000   470      scan\n8.1       8     desert    28000   460     strip\n8.2       8     desert    28000   460      scan\n8.3       8     desert    10000   470     strip\n8.4       8     desert    18000   450 spotlight\n8.5       8     desert    14000   550      scan\n8.6       8     desert    20000   550 spotlight\n8.7       8     desert    14000   550     strip\n8.8       8     desert    10000   470      scan\n8.9       8     desert    24000   510      scan\n8.10      8     desert    20000   550 spotlight\n8.11      8     desert    10000   470 spotlight\n8.12      8     desert    28000   460 spotlight\n\n\n\n\nDesign augmentation\nIn addition to split-plot designs, there are other occasions you might run into blocked designs. One of these occasions is when you’re using an iterative testing strategy referred to as sequential experimental design, which is an useful tool when designing resource-constrained experiments.\nSequential design refers to running a much smaller (and thus, less resource- intensive) screening design to test for the existence of effects before committing to a more expensive experiment to characterize their size. After running a screening experiment and detecting that some effects are active, a practitioner needs to design a follow-up experiment to actually characterize the active effects. Generating an entire new optimal design ignores the information already collected in the prior experiment, potentially wasting testing resources that can be targeted at characterizing the effects of interest.\nskpr’s Alias-optimality criteria can generate optimal screening designs. Let’s see how it does.\n\nset.seed(2023)\nflight_design_screening = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ altitude + speed + mode,\n                           trials = 12, optimality = \"alias\", minDopt = 0, repeats = 100,\n                           parallel = TRUE) \nset.seed(2023)\nflight_design_d = gen_design(candidateset = constrained_candidate_set,\n                           model = ~ altitude + speed + mode,\n                           trials = 12, optimality = \"D\", repeats=100) \n\nggplot(flight_design_screening) +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\nggplot(flight_design_d) +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n#Comparing optimality values\nget_optimality(flight_design_d)\n\n         D         I        A            G        T        E    Alias\n1 83.36936 0.3515361 78.41141 Not Computed 52.64198 5.532092 4.767266\n\nget_optimality(flight_design_screening)\n\n         D         I        A            G        T        E    Alias\n1 67.50859 0.4056887 58.13202 Not Computed 45.42741 3.294533 4.008502\n\n#Comparing correlation structure\nplot_correlations(flight_design_d)\n\n\n\nplot_correlations(flight_design_screening)\n\n\n\n\nNow that we have our screening design–let’s augment it. Let’s say we ran our experiment and found that speed:mode and altitude:speed aren’t significant. We can then remove that term from the design and generate a new design that only consists of the terms we have found to be active.\n\ngen_design(constrained_candidate_set, \n           model = ~altitude + speed + mode + altitude:mode,\n           augmentdesign = flight_design_screening,\n           trials = 24) ->\n  augmented_design\n\nWarning in model.matrix.default(model, augmentnormalized, contrasts.arg =\ncontrastslist): variable 'target_env' is absent, its contrast will be ignored\n\naugmented_design\n\n   Block1 altitude speed      mode\n1       1    17000   455      scan\n2       1    13000   515      scan\n3       1    13000   525 spotlight\n4       1    13000   525     strip\n5       1    21000   540     strip\n6       1    26000   485     strip\n7       1    17000   450 spotlight\n8       1    26000   485 spotlight\n9       1    17000   450     strip\n10      1    20000   550      scan\n11      1    21000   540 spotlight\n12      1    26000   480      scan\n13      2    28000   460      scan\n14      2    10000   470 spotlight\n15      2    10000   470     strip\n16      2    10000   470      scan\n17      2    28000   460     strip\n18      2    28000   460 spotlight\n19      2    10000   470      scan\n20      2    28000   460      scan\n21      2    28000   460 spotlight\n22      2    10000   470 spotlight\n23      2    10000   470     strip\n24      2    28000   460     strip\n\n#Dark green dots are the new points\nggplot() +\n  geom_point(data=constrained_candidate_set, aes(x=speed,y=altitude),size=0.5) +\n  geom_count(data=augmented_design, aes(x=speed,y=altitude),color=\"darkgreen\") +\n  geom_count(data=flight_design_screening, aes(x=speed,y=altitude),color=\"red\") +\n  facet_wrap(~mode) +\n  scale_radius(range = c(2,5), breaks = (function(x) seq(min(x),max(x),by=1)))\n\n\n\n\n\n\nStatistical Power\nOne of the most common goals in running an experiment is to study how a response variable is affected by changes in the experimental factors, and whether those changes are statistically significant. This means the primary concern of the experimenter is to make sure their experiment can actually detect these effects if they exist. We judge this primarily based on statistical power: the probability that our experiment will be able to detect an effect if one truly exists. We’ll look into how to calculate that in skpr next.\nBut first, let’s regenerate and save our D-optimal design to a CSV file. Although we can easy regenerate our design later by setting our random seed and calling gen_design() again, it can also be nice to save it to a file so you can easily share it and load it later.\n\n\nSaving design to CSV to share\n\nset.seed(2023)\ndesign_final = gen_design(candidateset = constrained_candidate_set,\n                          model = ~ (altitude + speed + mode)^2 + I(speed^2) + I(altitude^2),\n                          trials = 72) \nwrite.csv(design_final, file = \"design_final.csv\", row.names = FALSE)"
  }
]